name: Deploy Multi License API (compose, static creds) – fixed ECR URL

on:
  workflow_dispatch:
    inputs:
      ENV:
        description: "Environment to deploy (dev/staging/prod)"
        required: true
        default: "stage"
      ROLLBACK_TAG:
        description: "Optional: previous tag to deploy (e.g., 2025.06.30.02). If blank, deploys new build."
        required: false

env:
  # Set this to the region where your EC2/ECR actually live (recommend ap-south-1 for your setup)
  AWS_REGION: ${{ vars.AWS_REGION || 'ap-south-1' }}
  TF_REGION:  ${{ vars.AWS_REGION || 'ap-south-1' }}
  # Optional: override repo name if not ${ENV}-idlms-api
  ECR_REPO_NAME: ${{ vars.ECR_REPO_NAME }}

jobs:
  deploy:
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.ENV || 'stage' }}
    permissions:
      contents: read

    steps:
      - name: Checkout code (feature/btl-77)
        uses: actions/checkout@v4
        with:
          ref: feature/btl-77

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.5.0

      - name: Configure AWS Credentials (static keys)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Who am I?
        run: aws sts get-caller-identity

      - name: Get AWS Account ID
        id: aws-account
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          echo "account_id=$ACCOUNT_ID" >> "$GITHUB_OUTPUT"

      - name: Set dynamic TF & backup buckets
        run: |
          echo "TF_BUCKET=${{ github.event.inputs.ENV }}-btl-idlms-backend-api-tfstate-${{ steps.aws-account.outputs.account_id }}" >> $GITHUB_ENV
          echo "BACKUP_BUCKET=idlms-${{ github.event.inputs.ENV }}-built-artifact-${{ steps.aws-account.outputs.account_id }}" >> $GITHUB_ENV
          echo "Using TF_BUCKET=$TF_BUCKET"
          echo "Using BACKUP_BUCKET=$BACKUP_BUCKET"

      # ───────── Optional infra applies — SKIP if folder missing ─────────
      - name: Terraform Apply VPC (skip if folder missing)
        run: |
          if [ ! -d infra/vpc ]; then echo "infra/vpc not found; skipping"; exit 0; fi
          cd infra/vpc
          terraform init \
            -backend-config="bucket=${TF_BUCKET}" \
            -backend-config="key=${{ github.event.inputs.ENV }}/vpc/terraform.tfstate" \
            -backend-config="region=${TF_REGION}"
          terraform apply -auto-approve -var-file="${{ github.event.inputs.ENV }}.tfvars"

      - name: Terraform Apply NLB (skip if folder missing)
        run: |
          if [ ! -d infra/nlb ]; then echo "infra/nlb not found; skipping"; exit 0; fi
          cd infra/nlb
          terraform init \
            -backend-config="bucket=${TF_BUCKET}" \
            -backend-config="key=${{ github.event.inputs.ENV }}/nlb/terraform.tfstate" \
            -backend-config="region=${TF_REGION}"
          terraform apply -auto-approve -var-file="${{ github.event.inputs.ENV }}.tfvars"

      - name: Terraform Apply API Gateway (skip if folder missing)
        run: |
          if [ ! -d infra/rest-api ]; then echo "infra/rest-api not found; skipping"; exit 0; fi
          cd infra/rest-api
          terraform init \
            -backend-config="bucket=${TF_BUCKET}" \
            -backend-config="key=${{ github.event.inputs.ENV }}/rest-api/terraform.tfstate" \
            -backend-config="region=${TF_REGION}"
          terraform refresh -var-file="${{ github.event.inputs.ENV }}.tfvars"
          terraform apply -auto-approve -var-file="${{ github.event.inputs.ENV }}.tfvars"

      - name: Terraform Apply CloudWatch (skip if folder missing)
        run: |
          if [ ! -d infra/cloudwatch ]; then echo "infra/cloudwatch not found; skipping"; exit 0; fi
          cd infra/cloudwatch
          terraform init \
            -backend-config="bucket=${TF_BUCKET}" \
            -backend-config="key=${{ github.event.inputs.ENV }}/cloudwatch/terraform.tfstate" \
            -backend-config="region=${TF_REGION}"
          terraform apply -auto-approve -var-file="${{ github.event.inputs.ENV }}.tfvars"

      - name: Terraform Apply ECR (capture repo URL) (skip if folder missing)
        id: ecr
        run: |
          if [ ! -d infra/ecr ]; then echo "infra/ecr not found; skipping (will derive repo URL)"; exit 0; fi
          cd infra/ecr
          terraform init \
            -backend-config="bucket=${TF_BUCKET}" \
            -backend-config="key=${{ github.event.inputs.ENV }}/ecr/terraform.tfstate" \
            -backend-config="region=${TF_REGION}"
          terraform apply -auto-approve -var-file="${{ github.event.inputs.ENV }}.tfvars"
          ECR_REPO_URL=$(terraform output -raw ecr_repository_url 2>/dev/null || true)
          if [ -z "$ECR_REPO_URL" ]; then
            ECR_REPO_URL=$(terraform output -json repository_urls | jq -r '.[0]' 2>/dev/null || true)
          fi
          if [ -n "$ECR_REPO_URL" ] && [ "$ECR_REPO_URL" != "null" ]; then
            echo "ECR_REPO_URL=$ECR_REPO_URL" >> $GITHUB_ENV
            echo "Using ECR from Terraform: $ECR_REPO_URL"
          fi

      - name: Terraform Apply SSM (publish app env) (skip if folder missing)
        id: ssm
        run: |
          if [ ! -d infra/ssm ]; then echo "infra/ssm not found; skipping"; exit 0; fi
          cd infra/ssm
          PARAM_NAME="/idlms/shared/${{ github.event.inputs.ENV }}/.env"
          if aws ssm get-parameter --name "$PARAM_NAME" --with-decryption > /dev/null 2>&1; then
            ENV_CONTENT=$(aws ssm get-parameter --name "$PARAM_NAME" --with-decryption --query "Parameter.Value" --output text)
            BASE64_ENV=$(echo "$ENV_CONTENT" | base64 -w 0)
          else
            BASE64_ENV=$(echo "# placeholder env" | base64 -w 0)
          fi
          terraform init \
            -backend-config="bucket=${TF_BUCKET}" \
            -backend-config="key=${{ github.event.inputs.ENV }}/ssm/terraform.tfstate" \
            -backend-config="region=${TF_REGION}"
          terraform apply -auto-approve -var-file="${{ github.event.inputs.ENV }}.tfvars" -var="app_env_content=${BASE64_ENV}"
          SSM_ENV_PARAM=$(terraform output -raw ssm_env_param_name | tr -d '\r\n' | sed 's/^ssm:\/\///')
          echo "SSM_ENV_PARAM=$SSM_ENV_PARAM" >> $GITHUB_ENV

      - name: Terraform Apply S3 (skip if folder missing)
        id: s3_apply
        run: |
          if [ ! -d infra/s3 ]; then echo "infra/s3 not found; skipping"; exit 0; fi
          cd infra/s3
          terraform init \
            -backend-config="bucket=${TF_BUCKET}" \
            -backend-config="key=${{ github.event.inputs.ENV }}/s3/terraform.tfstate" \
            -backend-config="region=${TF_REGION}"
          terraform apply -auto-approve -var-file="${{ github.event.inputs.ENV }}.tfvars"

      # ───────── Build & push (compose + guaranteed fallback) ─────────
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Generate build tag
        id: tags
        run: |
          DATE_TAG=$(date +'%Y.%m.%d')
          BUILD_NUM=$(printf "%03d" $GITHUB_RUN_NUMBER)
          BUILD_TAG="${DATE_TAG}.${BUILD_NUM}"
          echo "BUILD_TAG=$BUILD_TAG" >> $GITHUB_ENV

      - name: Determine ECR repo URL (robust; tries other region if needed)
        run: |
          set -euo pipefail
          # 1) If Terraform already set it, keep it
          if [ -n "${ECR_REPO_URL:-}" ] && [ "${ECR_REPO_URL}" != "null" ]; then
            echo "Keeping ECR_REPO_URL from Terraform: ${ECR_REPO_URL}"
            # infer ECR_REGION from URL
            ECR_REGION="$(echo "$ECR_REPO_URL" | awk -F. '{print $(NF-2) "-" $(NF-1)}' | sed 's/^dkr-ecr-//; s/amazonaws//; s/.$//')"
            [ -z "$ECR_REGION" ] && ECR_REGION="${AWS_REGION}"
            echo "ECR_REGION=${ECR_REGION}" >> $GITHUB_ENV
            exit 0
          fi

          # 2) If repo variable exists, use it
          if [ -n "${{ vars.ECR_REPO_URL || '' }}" ]; then
            echo "ECR_REPO_URL=${{ vars.ECR_REPO_URL }}" >> $GITHUB_ENV
            # derive region from url
            URL="${{ vars.ECR_REPO_URL }}"
            ECR_REGION="$(echo "$URL" | awk -F. '{print $(NF-2)}')"
            [ -z "$ECR_REGION" ] && ECR_REGION="${AWS_REGION}"
            echo "ECR_REGION=${ECR_REGION}" >> $GITHUB_ENV
            exit 0
          fi

          # 3) Derive from account+region+repo name, and if not found in $AWS_REGION, try the other common region
          ACCOUNT_ID="${{ steps.aws-account.outputs.account_id }}"
          REPO_NAME="${ECR_REPO_NAME:-${{ github.event.inputs.ENV }}-idlms-api}"

          # First try the declared AWS_REGION
          ECR_REGION="${AWS_REGION}"
          if ! aws ecr describe-repositories --repository-names "$REPO_NAME" --region "$ECR_REGION" >/dev/null 2>&1; then
            # Try the other region commonly used in your logs
            ALT_REGIONS="ap-south-1 ap-southeast-1"
            for R in $ALT_REGIONS; do
              if [ "$R" != "$ECR_REGION" ] && aws ecr describe-repositories --repository-names "$REPO_NAME" --region "$R" >/dev/null 2>&1; then
                ECR_REGION="$R"
                break
              fi
            done
          fi

          if ! aws ecr describe-repositories --repository-names "$REPO_NAME" --region "$ECR_REGION" >/dev/null 2>&1; then
            echo "::error::ECR repository '$REPO_NAME' not found in $AWS_REGION or fallback regions. Create it (via infra/ecr or console) or set repo variable ECR_REPO_URL."
            exit 1
          fi

          ECR_REPO_URL="${ACCOUNT_ID}.dkr.ecr.${ECR_REGION}.amazonaws.com/${REPO_NAME}"
          echo "ECR_REPO_URL=$ECR_REPO_URL" >> $GITHUB_ENV
          echo "ECR_REGION=$ECR_REGION"       >> $GITHUB_ENV
          echo "Resolved ECR_REPO_URL=$ECR_REPO_URL (region $ECR_REGION)"

      - name: Build & push via docker compose if images are defined
        env:
          IMAGE_REPO: ${{ env.ECR_REPO_URL }}
          BUILD_TAG:  ${{ env.BUILD_TAG }}
        run: |
          set -euo pipefail
          if [ -f docker/docker-compose.yml ] && grep -qE '^\s*image:' docker/docker-compose.yml; then
            echo "Found image fields in compose; building & pushing with compose"
            export IMAGE_REPO BUILD_TAG
            # Render compose to pin image to ECR_REPO_URL:BUILD_TAG
            sed -E "s|(image:\\s*)([^:]+)(:)?(\\$\\{BUILD_TAG\\})?|\\1${IMAGE_REPO}:\\${BUILD_TAG}|g" docker/docker-compose.yml > docker/docker-compose.rendered.yml
            docker compose -f docker/docker-compose.rendered.yml build
            docker compose -f docker/docker-compose.rendered.yml push || true
          else
            echo "No 'image:' in compose; will rely on single-image build below."
          fi

      - name: Canonical single-image build & push (fallback/guarantee)
        env:
          ECR_REPO_URL: ${{ env.ECR_REPO_URL }}
          BUILD_TAG:    ${{ env.BUILD_TAG }}
          ECR_REGION:   ${{ env.ECR_REGION || env.AWS_REGION }}
        run: |
          set -euo pipefail
          IMAGE_URI="${ECR_REPO_URL}:${BUILD_TAG}"
          LATEST_URI="${ECR_REPO_URL}:latest"
          echo "IMAGE_URI=$IMAGE_URI"   >> $GITHUB_ENV
          echo "LATEST_URI=$LATEST_URI" >> $GITHUB_ENV
          # login and push in the ECR's region
          aws ecr get-login-password --region "${ECR_REGION}" | docker login --username AWS --password-stdin "$(echo "$ECR_REPO_URL" | cut -d/ -f1)"
          docker build -t "$IMAGE_URI" -t "$LATEST_URI" -f docker/Dockerfile src
          docker push "$IMAGE_URI"
          docker push "$LATEST_URI"

      - name: Decide tag to deploy
        id: determine-tag
        run: |
          TAG="${{ github.event.inputs.ROLLBACK_TAG }}"
          if [ -z "$TAG" ]; then TAG="${{ env.BUILD_TAG }}"; fi
          echo "TAG_TO_DEPLOY=$TAG" >> $GITHUB_ENV
          echo "Deploying tag: $TAG"

      - name: Upload docker-compose.yml to S3 (rendered with correct ECR repo)
        run: |
          SRC="docker/docker-compose.rendered.yml"
          if [ ! -f "$SRC" ]; then SRC="docker/docker-compose.yml"; fi
          aws s3 cp "$SRC" "s3://${BACKUP_BUCKET}/${{ github.event.inputs.ENV }}/docker-compose.yml"

      # ───────── Deploy on EC2 via SSM (compose + rollback) ─────────
      - name: Deploy containers with rollback logic via SSM
        run: |
          TAG_TO_DEPLOY="${{ env.TAG_TO_DEPLOY }}"
          ENV="${{ github.event.inputs.ENV }}"
          ECR_REPO_URL="${{ env.ECR_REPO_URL }}"
          ECR_REGION="${{ env.ECR_REGION || env.AWS_REGION }}"
          AWS_REGION="${{ env.AWS_REGION }}"
          BACKUP_BUCKET="${{ env.BACKUP_BUCKET }}"

          INSTANCE_ID=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=Backend API IDLMS-${ENV}" "Name=instance-state-name,Values=running" \
            --query "Reservations[].Instances[].InstanceId" \
            --region "$AWS_REGION" \
            --output text)

          if [ -z "$INSTANCE_ID" ]; then
            echo "ERROR: No running EC2 instance found for environment $ENV"
            exit 1
          fi

          aws ssm send-command \
            --document-name "AWS-RunShellScript" \
            --instance-ids "$INSTANCE_ID" \
            --comment "Deploy containers with rollback logic" \
            --parameters 'commands=[
              "set -e",
              "cd /home/ubuntu",
              "ENV_CONTENT=$(aws ssm get-parameter --name \"/idlms/shared/'"$ENV"'/.env\" --with-decryption --query \"Parameter.Value\" --output text)",
              "echo \"$ENV_CONTENT\" > .env",
              "echo \"BUILD_TAG='"$TAG_TO_DEPLOY"'\" >> .env",
              "echo \"IMAGE_REPO='"$ECR_REPO_URL"'\" >> .env",
              "aws s3 cp s3://'"$BACKUP_BUCKET"'/'"$ENV"'/docker-compose.yml docker-compose.yml",
              "if ! command -v docker &> /dev/null; then sudo apt-get update -y && sudo apt-get install -y docker.io; fi",
              "if ! command -v docker-compose &> /dev/null; then curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose && chmod +x /usr/local/bin/docker-compose && ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose || true; fi",
              "aws ecr get-login-password --region '"$ECR_REGION"' | docker login --username AWS --password-stdin '"${ECR_REPO_URL%/*}"'",
              "docker-compose --env-file .env down || true",
              "docker rmi ${IMAGE_REPO}:${BUILD_TAG} || true",
              "docker-compose --env-file .env pull --ignore-pull-failures",
              "docker-compose --env-file .env up -d --force-recreate --build",
              "sleep 60",
              "RUNNING_CONTAINERS=$(docker ps --format '{{.Names}}' | wc -l) && \
              if [ \\\"$RUNNING_CONTAINERS\\\" -lt 1 ]; then \
                echo \\\"Deployment failed. Rolling back...\\\"; \
                PREV_TAG=$(aws ssm get-parameter --name \\\"/idlms/license-api/last-successful-build\\\" --query \\\"Parameter.Value\\\" --output text); \
                echo \\\"Rolling back to tag: $PREV_TAG\\\"; \
                sed -i \\\"/BUILD_TAG=/d\\\" .env; \
                echo \\\"BUILD_TAG=$PREV_TAG\\\" >> .env; \
                docker-compose --env-file .env down || true; \
                docker rmi ${IMAGE_REPO}:${BUILD_TAG} || true; \
                docker-compose --env-file .env pull --ignore-pull-failures; \
                docker-compose --env-file .env up -d --force-recreate --build; \
              else \
                echo \\\"All containers are up. Saving $TAG_TO_DEPLOY as last-successful-build...\\\"; \
                aws ssm put-parameter --name \\\"/idlms/license-api/last-successful-build\\\" --value \\\"$TAG_TO_DEPLOY\\\" --type String --overwrite; \
              fi"
            ]' \
            --timeout-seconds 900 \
            --region "$AWS_REGION" \
            --output text
